@article{Jing2021,
  abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  author = {Jing, Longlong and Tian, Yingli},
  doi = {10.1109/TPAMI.2021.3097442},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {type:self_supervised_learning, survey, computer_vision, deep_learning},
  number = {11},
  pages = {4037--4058},
  publisher = {IEEE},
  title = {Self-Supervised Visual Feature Learning with Deep Neural Networks: A Survey},
  url = {https://ieeexplore.ieee.org/document/9457623},
  volume = {43},
  year = {2021}
}

@inproceedings{Zhang2022,
  abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose ConVIRT, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test ConVIRT by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
  author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
  booktitle = {Proceedings of the 7th Machine Learning for Healthcare Conference},
  keywords = {type:self_supervised_learning, contrastive_learning, medical_imaging, multimodal},
  pages = {2--25},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
  url = {https://proceedings.mlr.press/v182/zhang22a.html},
  volume = {182},
  year = {2022}
}

@inproceedings{Chen2020,
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  keywords = {type:self_supervised_learning, contrastive_learning, representation_learning},
  pages = {1597--1607},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  url = {https://proceedings.mlr.press/v119/chen20j.html},
  volume = {119},
  year = {2020}
}

@inproceedings{Grill2020,
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6\% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and Kavukcuoglu, Koray and Munos, Remi and Valko, Michal},
  booktitle = {Advances in Neural Information Processing Systems},
  keywords = {type:self_supervised_learning, representation_learning, byol},
  pages = {21271--21284},
  publisher = {Curran Associates, Inc.},
  title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
  url = {https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
  volume = {33},
  year = {2020}
}

@inproceedings{Zbontar2021,
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  keywords = {type:self_supervised_learning, redundancy_reduction, barlow_twins},
  pages = {12310--12320},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  url = {https://proceedings.mlr.press/v139/zbontar21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{He2020,
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {type:self_supervised_learning, contrastive_learning, momentum_contrast},
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
  year = {2020}
}

@inproceedings{Devlin2019,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  keywords = {type:self_supervised_learning, bert, nlp, transformers, pre_training},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {https://aclanthology.org/N19-1423/},
  year = {2019}
}

@inproceedings{Baevski2022,
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  keywords = {type:self_supervised_learning, multimodal, transformers},
  pages = {1298--1312},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
  url = {https://proceedings.mlr.press/v162/baevski22a.html},
  volume = {162},
  year = {2022}
}

@article{Mascolini2022,
  abstract = {MOTIVATION: Computer-aided analysis of biological images typically requires extensive training on large-scale annotated datasets, which is not viable in many situations. In this paper, we present Generative Adversarial Network Discriminator Learner (GAN-DL), a novel self-supervised learning paradigm based on the StyleGAN2 architecture, which we employ for self-supervised image representation learning in the case of fluorescent biological images. RESULTS: We show that Wasserstein Generative Adversarial Networks enable high-throughput compound screening based on raw images. We demonstrate this by classifying active and inactive compounds tested for the inhibition of SARS-CoV-2 infection in two different cell models: the primary human renal cortical epithelial cells (HRCE) and the African green monkey kidney epithelial cells (VERO). In contrast to previous methods, our deep learning-based approach does not require any annotation, and can also be used to solve subtle tasks it was not specifically trained on, in a self-supervised manner. For example, it can effectively derive a dose-response curve for the tested treatments.},
  author = {Mascolini, Alessio and Cardamone, Dario and Ponzio, Francesco and Di Cataldo, Santa and Ficarra, Elisa},
  doi = {10.1186/s12859-022-04823-7},
  journal = {BMC Bioinformatics},
  keywords = {type:self_supervised_learning, generative_models, bioimaging},
  number = {1},
  pages = {295},
  publisher = {Springer},
  title = {Exploiting Generative Self-Supervised Learning for the Assessment of Biological Images with Lack of Annotations},
  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04823-7},
  volume = {23},
  year = {2022}
}

@misc{noauthor_undated,
  title        = {Improving Language Understanding with Unsupervised Learning},
  abstract     = {We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets.},
  howpublished = {\url{https://openai.com/research/language-unsupervised}},
  keywords     = {type:unsupervised_learning, nlp, transformers}
}

@article{Liu2025,
  abstract  = {Self-supervised learning (SSL) automates the extraction and interpretation of histopathology features on unannotated hematoxylin-eosin-stained whole slide images (WSIs). We train an SSL Barlow Twins encoder on 435 colon adenocarcinoma WSIs from The Cancer Genome Atlas to extract features from small image patches (tiles). Leiden community detection groups tiles into histomorphological phenotype clusters (HPCs). HPC reproducibility and predictive ability for overall survival are confirmed in an independent clinical trial (N = 1213 WSIs). This unbiased atlas results in 47 HPCs displaying unique and shared clinically significant histomorphological traits, highlighting tissue type, quantity, and architecture, especially in the context of tumor stroma. Through in-depth analyses of these HPCs, including immune landscape and gene set enrichment analyses, and associations to clinical outcomes, we shine light on the factors influencing survival and responses to treatments of standard adjuvant chemotherapy and experimental therapies. Further exploration of HPCs may unveil additional insights and aid decision-making and personalized treatments for colon cancer patients.},
  author    = {Liu, Bojing and Polack, Meaghan and Coudray, Nicolas and Claudio Quiros, Adalberto and Sakellaropoulos, Theodore and Le, Hortense and Karimkhan, Afreen and Crobach, Augustinus S. L. P. and van Krieken, J. Han J. M. and Yuan, Ke and Tollenaar, Rob A. E. M. and Mesker, Wilma E. and Tsirigos, Aristotelis},
  doi       = {10.1038/s41467-025-24605-8},
  journal   = {Nature Communications},
  keywords  = {type:self_supervised_learning, medical_imaging, histopathology},
  number    = {1},
  pages     = {2328},
  publisher = {Springer Nature},
  title     = {Self-Supervised Learning Reveals Clinically Relevant Histomorphological Patterns for Therapeutic Strategies in Colon Cancer},
}

@article{Liu2025,
  abstract  = {Self-supervised learning (SSL) automates the extraction and interpretation of histopathology features on unannotated hematoxylin-eosin-stained whole slide images (WSIs). We train an SSL Barlow Twins encoder on 435 colon adenocarcinoma WSIs from The Cancer Genome Atlas to extract features from small image patches (tiles). Leiden community detection groups tiles into histomorphological phenotype clusters (HPCs). HPC reproducibility and predictive ability for overall survival are confirmed in an independent clinical trial (N = 1213 WSIs). This unbiased atlas results in 47 HPCs displaying unique and shared clinically significant histomorphological traits, highlighting tissue type, quantity, and architecture, especially in the context of tumor stroma. Through in-depth analyses of these HPCs, including immune landscape and gene set enrichment analyses, and associations to clinical outcomes, we shine light on the factors influencing survival and responses to treatments of standard adjuvant chemotherapy and experimental therapies. Further exploration of HPCs may unveil additional insights and aid decision-making and personalized treatments for colon cancer patients.},
  author    = {Liu, Bojing and Polack, Meaghan and Coudray, Nicolas and Claudio Quiros, Adalberto and Sakellaropoulos, Theodore and Le, Hortense and Karimkhan, Afreen and Crobach, Augustinus S. L. P. and van Krieken, J. Han J. M. and Yuan, Ke and Tollenaar, Rob A. E. M. and Mesker, Wilma E. and Tsirigos, Aristotelis},
  doi       = {10.1038/s41467-025-24605-8},
  journal   = {Nature Communications},
  keywords  = {type:self_supervised_learning, medical_imaging, histopathology},
  number    = {1},
  pages     = {2328},
  publisher = {Springer Nature},
  title     = {Self-Supervised Learning Reveals Clinically Relevant Histomorphological Patterns for Therapeutic Strategies in Colon Cancer},
  volume    =  {16},
  year      =  {2025},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  language  = {en}
}


